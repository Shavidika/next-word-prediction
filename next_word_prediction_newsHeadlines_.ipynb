{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "literature-survey",
      "metadata": {
        "id": "literature-survey"
      },
      "source": [
        "# Literature Survey\n",
        "\n",
        "Next-word prediction is a key task in natural language processing that involves predicting the next word in a sentence based on the preceding words. This project aims to develop a next-word prediction model using state-of-the-art deep learning techniques. The model will be built using Recurrent Neural Networks (RNNs) and their advanced variant, Long Short-Term Memory (LSTM) networks, known for their ability to capture long-term dependencies in text.By training these models on large text datasets, we aim to achieve high accuracy in predicting the next word, making the model useful for applications such as text completion, chatbots, and virtual assistants. The project will involve data preprocessing, model implementation, training, evaluation, and fine-tuning to optimize performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dataset-description",
      "metadata": {
        "id": "dataset-description"
      },
      "source": [
        "# Dataset Description\n",
        "\n",
        "The dataset used in this project is sourced from Kaggle and contains news headlines from the month of March 2018.\n",
        "The dataset provides a rich source of textual data suitable for training a next-word prediction model.\n",
        "Each record in the dataset represents a single news headline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "import-libraries",
      "metadata": {
        "id": "import-libraries"
      },
      "outputs": [],
      "source": [
        "!pip install keras tensorflow\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zDa185doCyWk",
      "metadata": {
        "id": "zDa185doCyWk"
      },
      "source": [
        "### Install opendatasets and download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-dataset",
      "metadata": {
        "id": "load-dataset"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets -q\n",
        "import opendatasets as od"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VIC3j3Q3C2qz",
      "metadata": {
        "id": "VIC3j3Q3C2qz"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bGjeFfZa-u8-",
      "metadata": {
        "id": "bGjeFfZa-u8-"
      },
      "outputs": [],
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/manishguptads/news-headlines/code\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B0m6UQYYDKgx",
      "metadata": {
        "id": "B0m6UQYYDKgx"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exploratory-data-analysis",
      "metadata": {
        "id": "exploratory-data-analysis"
      },
      "outputs": [],
      "source": [
        "news_data = pd.read_csv(\"/content/news-headlines/ArticlesMarch2018.csv\")\n",
        "news_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "number-of-records",
      "metadata": {
        "id": "number-of-records"
      },
      "outputs": [],
      "source": [
        "print(\"Number of records: \", news_data.shape[0])\n",
        "print(\"Number of fields: \", news_data.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qTi7Y7eO_DJJ",
      "metadata": {
        "id": "qTi7Y7eO_DJJ"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "Exploratory Data Analysis (EDA) involves analyzing the dataset to summarize its main characteristics, often using visual methods.\n",
        "For this dataset, we can explore the distribution of headline lengths, the most common words, and other relevant statistics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wuh6SvdrDUiv",
      "metadata": {
        "id": "wuh6SvdrDUiv"
      },
      "source": [
        "### Display the first few records and some basic statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "headlines",
      "metadata": {
        "id": "headlines"
      },
      "outputs": [],
      "source": [
        "print(news_data['headline'].head())\n",
        "print(news_data['headline'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "data-preprocessing",
      "metadata": {
        "id": "data-preprocessing"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Data preprocessing is a crucial step in preparing the data for model training.\n",
        "It includes tasks such as removing unwanted characters, tokenizing the text, and creating sequences of words for model input."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G0QXaIaZ_ZZf",
      "metadata": {
        "id": "G0QXaIaZ_ZZf"
      },
      "source": [
        "### Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "clean-headlines",
      "metadata": {
        "id": "clean-headlines"
      },
      "outputs": [],
      "source": [
        "news_data['headline'] = news_data['headline'].apply(lambda x: x.replace(u'\\xa0',u' '))\n",
        "news_data['headline'] = news_data['headline'].apply(lambda x: x.replace('\\u200a',' '))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenization\n"
      ],
      "metadata": {
        "id": "XVJ4HO1qkvrs"
      },
      "id": "XVJ4HO1qkvrs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tokenizer",
      "metadata": {
        "id": "tokenizer"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(oov_token='<oov>') # For those words which are not found in word_index\n",
        "tokenizer.fit_on_texts(news_data['headline'])\n",
        "\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open('tokenizer.json', 'w') as file:\n",
        "    file.write(tokenizer_json)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Total number of words: \", total_words)\n",
        "print(\"Word: ID\")\n",
        "print(\"------------\")\n",
        "print(\"<oov>: \", tokenizer.word_index['<oov>'])\n",
        "print(\"Strong: \", tokenizer.word_index['strong'])\n",
        "print(\"And: \", tokenizer.word_index['and'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yzrkz2dS_w63",
      "metadata": {
        "id": "yzrkz2dS_w63"
      },
      "source": [
        "### Creating sequences of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-sequences",
      "metadata": {
        "id": "create-sequences"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "for line in news_data['headline']:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "print(\"Total input sequences: \", len(input_sequences))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ha3K91ll__IK",
      "metadata": {
        "id": "ha3K91ll__IK"
      },
      "source": [
        "### Pad sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pad-sequences",
      "metadata": {
        "id": "pad-sequences"
      },
      "outputs": [],
      "source": [
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "max_sequence_len\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "input_sequences[1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(input_sequence) for input_sequence in input_sequences])\n",
        "max_length"
      ],
      "metadata": {
        "id": "Jz8FRLlpJbq7"
      },
      "id": "Jz8FRLlpJbq7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "xKlNLSS6AEgP",
      "metadata": {
        "id": "xKlNLSS6AEgP"
      },
      "source": [
        "### Create features and label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-features-labels",
      "metadata": {
        "id": "create-features-labels"
      },
      "outputs": [],
      "source": [
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "print-features-labels",
      "metadata": {
        "id": "print-features-labels"
      },
      "outputs": [],
      "source": [
        "print(xs[5])\n",
        "print(labels[5])\n",
        "print(ys[5][14])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model-implementation",
      "metadata": {
        "id": "model-implementation"
      },
      "source": [
        "# Model Implementation\n",
        "\n",
        "In this section, we implement the model for next-word prediction.\n",
        "We use a Bidirectional LSTM model with an Embedding layer and a Dense layer with a softmax activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uyjhrGvmAS6z",
      "metadata": {
        "id": "uyjhrGvmAS6z"
      },
      "source": [
        "### Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "define-model",
      "metadata": {
        "id": "define-model"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150)))\n",
        "model.add(Dense(total_words, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wPoe0MrIAczG",
      "metadata": {
        "id": "wPoe0MrIAczG"
      },
      "source": [
        "### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kj1XDIicAb58",
      "metadata": {
        "id": "kj1XDIicAb58"
      },
      "outputs": [],
      "source": [
        "adam = Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Kyf1nPvHAiVO",
      "metadata": {
        "id": "Kyf1nPvHAiVO"
      },
      "source": [
        "### Train the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train-model",
      "metadata": {
        "id": "train-model"
      },
      "outputs": [],
      "source": [
        "history = model.fit(xs, ys, epochs=50, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model-evaluation",
      "metadata": {
        "id": "model-evaluation"
      },
      "source": [
        "# Model Evaluation and Discussion\n",
        "\n",
        "Model evaluation involves assessing the performance of the model on the training data.\n",
        "We use accuracy and loss metrics to evaluate the model during training.\n",
        "We also visualize the training progress using plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hMP8zBeaA4ZI",
      "metadata": {
        "id": "hMP8zBeaA4ZI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aX-rRoXLA6tv",
      "metadata": {
        "id": "aX-rRoXLA6tv"
      },
      "outputs": [],
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hKQZNBXFA8_M",
      "metadata": {
        "id": "hKQZNBXFA8_M"
      },
      "outputs": [],
      "source": [
        "plot_graphs(history, 'accuracy')\n",
        "plot_graphs(history, 'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VvAOUkYtBeJ-",
      "metadata": {
        "id": "VvAOUkYtBeJ-"
      },
      "source": [
        "### Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "omiA8rpiBgSP",
      "metadata": {
        "id": "omiA8rpiBgSP"
      },
      "outputs": [],
      "source": [
        "model.save(\"next_word_prediction_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W2N-hzXrBk_3",
      "metadata": {
        "id": "W2N-hzXrBk_3"
      },
      "source": [
        "### Text Generation Function\n",
        "\n",
        "This function takes a seed text and generates the next words based on the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "evaluate-model",
      "metadata": {
        "id": "evaluate-model"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, seed_text, max_sequence_len, next_words):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        predicted = np.argmax(predicted, axis=1)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "### Example usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HtV9tIitB0Wn",
      "metadata": {
        "id": "HtV9tIitB0Wn"
      },
      "outputs": [],
      "source": [
        "seed_text = \"I am\"\n",
        "next_words = 5\n",
        "generated_text = generate_text(model, tokenizer, seed_text, max_sequence_len, next_words)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EQYnyoo3BJ1U",
      "metadata": {
        "id": "EQYnyoo3BJ1U"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this project, we developed a next-word prediction model using a Bidirectional LSTM.\n",
        "The model was trained on a dataset of news headlines, and we evaluated its performance using accuracy and loss metrics.\n",
        "The results indicate that the model is capable of predicting the next word in a given sequence with reasonable accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9p4Jr12EBSFo",
      "metadata": {
        "id": "9p4Jr12EBSFo"
      },
      "source": [
        "# References\n",
        "\n",
        "1. Gupta, M. (2021). News Headlines. Retrieved from https://www.kaggle.com/datasets/manishguptads/news-headlines/code"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}